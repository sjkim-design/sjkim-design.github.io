---
layout: post
title:  "머신러닝 알고리즘-장단점"
date:   2021-03-29T14:25:52-05:00
author: KSJ
categories: Algorithm
tags: lorem
---
1. 선형모델
- SVM과 로지스틱 회귀분석이
선형모델계열이다.

(1) 장점
- 학습속도 및 예측속도 빠름
- 방대 및 희소 데이터에도 잘 작동
- 계수를 통한 수식으로 변수의 영향도를 쉽게 파악할 수 있다.
(2) 단점
- 변수 간의 상관성이 높은 경우, 
계수 값이 불명확한 경우가 생긴다.
- 샘플에 비해 특성이 많을 때 잘 작동한다.

2. 나이브 베이즈 분류
(1) 장점
- 선형분류기보다 훈련/예측속도가 빠르다
- 고차원 데이터에서 잘 작동하며, 비교적 매개변수에 민감하지 않은 편이다.
(2) 단점
일반화 성능이 조금 뒤쳐진다.

3. 결정트리
(1) 개념: 
- 규칙을 순차적으로 적용하면서 독립변수 공간을 분할하는 분류모형
- CART(분류와 예측)을 수행
- 만들어진 모델을 쉽게 시각할 수 있음
(2) 작동원리
- 독립변수를 선택하여 독립변수에 대한 기준값을 정하여 해당 값 보다 큰 데이터그룹과 작은 데이터그룹으로 나눈다.
- 노드에 속한 데이터 클래스의 비율을 구하여 노드의 조건부 확률분포를 구하는데
이 조건부확률분포를 이용하여 클래스를 예측한다.
- 분류규칙은 부모와 자식 노드 간의 엔트로피를 가장 낮게 만드는 독립변수와 기준값을 찾는 방법이다. 이를 정량화한 것이 정보획득량(information gain)이라고 부른다.
- 모든 독립변수와 가능 기준값에 대한 정보획득량을 구하여 정보획득량이 가장 큰 독립변수와 기준값을 선택한다.
- 분할은 자식마디가 부모마디보다 순수도가 증가할 때 분류된다.
- 분류는 카이제곱통계량p value,지니계수, 엔트로피지수가 분류 기준으로 사용된다.
- 분류가 잘못될 경우의 가지를 제거한다.
(3) 종류
1) ID3
2) C.4.5
3) C.5.0
4) CART
5) CHAID

1)~3)은 기계학습 분야에서 개발/엔트로피, 정보획득량 개념 활용
4)~5)는 통계학 기초/카이스퀘어, T,F검정의 통계분석법 활용
(1) 장점
- if then 형식으로 룰을 이해하기 쉽다.
- 연속형, 범주형 변수 모두 활용 가능
- 변수의 중요도를 산출하여 비교 가능
- 비교적 속도가 빠름

(2) 단점
- 여러 변수를 동시에 고려하지 않고
한 변수를 선택하여 분류하기 때문에
단일 변수에서 차이를 구분하지 못할때 분류율이 떨어짐
- greedy algorithm(문제 해결과정에서
순간마다 최적이라고 생각되는 결정 방식을 택하여 최종 해답에 도달하는 문제해결 방식/계산속도가 큰 장점), Hill climbing(최적의 해를 찾아 값이 증가 또는 감소 방향으로 계속 이동하는 알고리즘)을 사용하는데
이 방식은 최적의 해를 보장하지는 않음
- 연속형 변수값 예측시 적당하지 않음
- 트리 모형이 복잡하면 예측력이 저하되고
해석 또한 어려워짐
- 데이터 변형에 민감, 레코드 개수의 차이에 따라 트리가 많이 달라질 수 있다. 비슷한 수준의 정보량을 가지는 두 변수가 있는데, 이 약간의 차이로 다른 변수가 선택될 경우, 트리 구성이 달라질 수 있다.

 의사결정나무관련http://blog.naver.com/trashx/60099037740
https://muzukphysics.tistory.com/138

전체 비교
https://dohk.tistory.com/170?fbclid=IwAR0GK5uZ6iTh0T2k1Mpz0e1MB9FpGo8EnxAYR9mY5vTDhhT010lCvK0Cg_k

https://wikidocs.net/
https://datascienceschool.net/03%20machine%20learning/12.01%20%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4.html
알고리즘 관련해서 강의가 잇는지
edwith



99. overfitting/underfitting 해결방법
- overfitting 과적합의 의미: 모델이 TRAIN SET에 과도하게 FITTING되어 TRAIN은 높은 정확도가 산출되지만
TEST SET에 대해서는 낮은 정확도를 보이는 현상입니다.

오버피팅은 high variance로
세부적인 특성까지 반영되어
학습된 특성은 예측을 잘하지만
학습되지 않은 특성의 경우, 예측을 잘하지 못합니다.

![over](https://user-images.githubusercontent.com/77030465/113381573-470a7580-93ba-11eb-8b3a-83becb157a4f.JPG)


- underfitting의 의미: 언더피팅은 high bias로 일부 특성만 반영
![under](https://user-images.githubusercontent.com/77030465/113381576-4c67c000-93ba-11eb-9399-2e50531ae51f.JPG)

![epoch1](https://user-images.githubusercontent.com/77030465/113385556-54782d80-93c3-11eb-8234-dbb9c9734923.JPG)

epoch 수가 증가함에 따라 신경망에서 가중치가 변경되고 
곡선이 과소 적합에서 최적에서 과적합 곡선으로 이동하는 횟수가 많아집니다.
epoch는 트리계열의 경우, 나무갯수 트리계열이 아닌 경우 학습횟수로 볼 수 있는데요.
적절한 epoch 수는 데이터셋의 특성에 따라 다릅니다.


- 언더피팅과 오버피팅의 관계
![언더피팅,오버피팅](https://user-images.githubusercontent.com/77030465/113381426-ea0ebf80-93b9-11eb-878b-a4f23b7fdf23.JPG)

모델링의 이상향은 
low bias로 실제값과 가까이 예측되어야 하고, low variance입니다.
TRAIN의 LOSS와 VALIDATION LOSS 모두 적고, 이 둘의 차이도 적은 모델이 GOOD FIT 모델이라고
할 수 있습니다.

또한 이러한 GOOD FIT 모델을 찾는 방법 중 하나가 EARLY STOPPING 방법인데요.
VALIDATION SET의 LOSS가 감소하다가 증가하는 시점에 나무의 갯수를 늘리는 것을 
중단하는 방법입니다. 보통은 그래프가 파동이 있으므로 증가하는 바로 첫 시점에 멈추는 것이 아니라
STOPPING_ROUNDS를 몇회 이상 감소하지 않을 경우 중단한다는 식으로 지정할 수 있습니다.


오버피팅 해결방법

- 오버피팅 모니터링:
VALIDATION의 오차가 감소하다가 증가하는 시점이 과적합이며, 이와 관련해서 early stopping을 사용할 수 있다. valid loss와 valid loss의 차이가 크면 좋지 않으며 수렴하는 것이 좋습니다.


1) 가중치 규제(regularization)
매개변수가 많은 복잡한 모델이 과적합 가능성이 높은데요. 복잡 모델을 방지하기 위해 L1규제(Lasso/절대값)/L2규제(Ridge/제곱) 등의 가중치 규제 방법으로 패널티를 주는 방법을 사용합니다.

regularization은 loss 감소에 기여하는 정도가 낮은 것은 0(L1)이나 0에 가까운 값(L2)으로 만듭니다.보통은 L2가 L1규제보다 많이 사용한다고 하네요. 


2) 데이터 증가
데이터의 양이 적을 경우, 데이터 패턴과 노이즈를 모두 학습하므로 과적합 가능성이 높아집니다. 데이터가 늘어날수록 해당 데이터의 일반적인 패턴을 학습하여 과적합을 방지할 수 있다고 합니다.

데이터가 적은 경우, 기존 데이터를 변형/추가하여 데이터의 양을 늘리기도 하는데 이를 데이터 증식 또는 증강이라고 부른다고 합니다. 이미지 학습이나 FDS탐지의 경우 이런 데이터 증식/증강을 사용합니다.

3) 드롭아웃(feature수 줄이기/노드랜덤선택):
- 핵심정보를 제외하고 일부변수를 포기
- 노드 중 일부만 랜덤 선택하여 활성화
지엽적인 특징으로 결과값이 산출되지 않도록 방지합니다. 

만개의 변수를 사용한 것과 100개의 중요변수를 사용한 모델의 성능이 유사한 경우 
100개의 변수를 쓰는 것이 낫습니다. 중요하지 않은 변수를 끌어다 사용하면서 TRAIN SET을 어떻게든 맞추려는 현상을 줄일 수 있습니다.


reference:
1) 오버피팅/언더피팅:
https://ai.plainenglish.io/solving-underfitting-and-overfitting-817f4e62bfa5
2. 과적합 방지: https://velog.io/@yookyungkho/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9D%98-%EA%B3%A0%EC%A7%88%EB%B3%91-Overfitting%EA%B3%BC%EC%A0%81%ED%95%A9-%ED%95%B4%EA%B2%B0-%ED%8C%81



